import requests
from bs4 import BeautifulSoup as bs
from Form_CSV import write_in_file
from headers import headers

URL_GL = 'https://grandline.ru'
filename_input = 'Grandline/input_GL.txt'
FILENAME = 'data_grandline.csv'


def parse_input():
    with open(filename_input) as file:
        input_arr = file.readlines()
        input_arr = [line.rstrip() for line in input_arr]
    return input_arr

def parse_product_page(url_full):
    products_arr = []
    r = requests.get(url_full, headers=headers)
    soup = bs(r.text, "html.parser")
    name = soup.find('h1', class_='h1').text

    specs = ''
    if soup.find('ul', class_='full-specifications-list'):
        specs_table = str(soup.find('ul', class_='full-specifications-list'))
        specs = '<table class="table table-striped table-condensed" <tbody>>' + specs_table + '</table> </tbody>'


    image = soup.find('span', class_='product-slider__img').find('img').get('src')


    about = ''
    if soup.find('div', class_='description'):
        about = soup.find('div', class_='description')
        for x in about.select('a'):
            x.decompose()

    img_about_url = ''

    documents_div = soup.find('ul', class_='documents-list').find_all('li')
    documents = ''
    for d in documents_div:
        documents += d.find('a').get('href') + '|'
        # documents.append(d.get('href'))
    price = ''
    products_arr.append(['', name, specs, image, about, '', price, '', documents, '', ''])
    print(products_arr)
    return products_arr

def parse_group(url):
    r = requests.get(url, headers=headers)
    soup = bs(r.text, "html.parser")

    category = soup.find_all('a', class_='breadcrumbs__link')[-3].find('span').text
    group = soup.find_all('a', class_='breadcrumbs__link')[-1].find('span').text
    brand = 'Grand Line'

    full_category = category + '>' + brand

    all_products = soup.find_all('li', class_='product-item')


    products = ['', '', '', '', '', '', '', '', '', category, group, brand]
    for product in all_products:
        product_url = product.find('a').get('href')
        product_data = parse_product_page(URL_GL+product_url)
        product_data.append([category, group, brand])
        products.append(product_data)

    return products

def parse_category(url):
    r = requests.get(url, headers=headers)
    soup = bs(r.text, "html.parser")
    matrix =[]
    group_urls = soup.find_all('li', class_='topic-item')
    for group_url_el in group_urls:
        group_url = group_url_el.find('a').get('href')
        full_gr_url = URL_GL + group_url
        mat = parse_group(full_gr_url)
        print(full_gr_url)
        matrix.append(mat)
    return matrix


def image_download(short_urls):
    for url in short_urls:
        image_url = URL_GL + url
        img_data = requests.get(image_url).content
        with open('img_gl/' + url, 'wb') as handler:
            handler.write(img_data)



if __name__ == '__main__':
    matrix = []
    for f in parse_input():
        matrix.append(parse_category(f))
    print(matrix)
    write_in_file(FILENAME, matrix)
    image_download(matrix[3])


